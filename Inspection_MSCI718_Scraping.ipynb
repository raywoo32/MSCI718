{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BioHacks2021_WebScraper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d86zy9Z7ePiB"
      },
      "source": [
        "# Augmenting Covid19 Data for Longterm Care Homes\n",
        "\n",
        "This is my webscraper, which will scrape data related to:\n",
        "1. Home-type, for profit status\n",
        "2. Number of non-compliances in most recent annual inspection \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pExG1dfLL63a"
      },
      "source": [
        "### Part 1: Scraping Home-type and accrediation data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAUESVEYeLPc"
      },
      "source": [
        "# Imports \n",
        "# !pip install beautifulsoup4\n",
        "import requests \n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buOpm2UchZK_"
      },
      "source": [
        "# Get initial data \n",
        "\n",
        "with open('HomeIDs.csv') as csv_file:\n",
        "    reader = csv.reader(csv_file, delimiter=',')\n",
        "    homeIDs = list(reader)[0]\n",
        "    homeIDs.remove('')\n",
        "    #print(len(homeIDs))\n",
        "\n",
        "#There are 514 LTC homes "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swvRJFVOf4fQ"
      },
      "source": [
        "# Webscraper 1: For profit status, and home type\n",
        "# DO NOT RUN!!! \n",
        "\n",
        "url_part1 = \"http://publicreporting.ltchomes.net/en-ca/homeprofile.aspx?Home=\"\n",
        "url_part3 = \"&tab=0\"\n",
        "\n",
        "rows = []\n",
        "notParsed = []\n",
        "for id in homeIDs:\n",
        "  print(\"Working on\", id)\n",
        "  URL = url_part1 + id + url_part3\n",
        "  page = requests.get(URL)\n",
        "  #Not successful query\n",
        "  if page.status_code != 200:\n",
        "    print(\"Error fetching page\", id)\n",
        "    notParsed.append(id)\n",
        "  #Successful query\n",
        "  else:\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "    all = soup.find_all(class_=\"Profilerow_col2\")\n",
        "    if (all == []):\n",
        "      print(\"Error wt idn page\", id)\n",
        "      notParsed.append(id)\n",
        "    else:\n",
        "      homeType = all[5].getText()\n",
        "      ResCouncil = all[8].getText()\n",
        "      FamCouncil = all[9].getText()\n",
        "      Accreditation = all[10].getText()\n",
        "      row = [id, homeType, ResCouncil, FamCouncil, Accreditation]\n",
        "      print(row)\n",
        "      rows.append(row)\n",
        "\n",
        "print(len(notParsed), notParsed)\n",
        "print(len(rows))\n",
        "#page = requests.get(URL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eMJIA_jo-Eq"
      },
      "source": [
        "# Save Webscraper 1 data to csv and download \n",
        "\n",
        "#LTC_accreditation\n",
        "with open('test.csv', 'a') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"LTCH_Num\", \"homeType\", \"ResCouncil\", \"FamCouncil\", \"Accreditation\"])\n",
        "    for row in rows:\n",
        "      writer.writerow(row)\n",
        "\n",
        "files.download('test.csv') \n",
        "print(notParsed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rSMcJrmvUTq"
      },
      "source": [
        "# Manually looked up codes for codes that didn't work \n",
        "\n",
        "nonParsed = ['2983', '3052', '956', '2985', '2986', '2987', '958', '2988', '925',  '922',  '907', '2992', '3023', '3029', '3050', '3034', '2993', '3031', '2995', '901', '2996', '2997', '3035', '965', '3000', '3016', '3002', '3026', '3003', '3004', '909', '2990', '962', '3006', '3005', '2991', '3033', '3013', '3014', '943', '3009', '3028', '3012', '3011', '3015', '3051', 'M633', '995', '3025', '3021', '3017', '3001', '963', '3018', '3020']\n",
        "toParse =   ['C501', '2647', '0956','C506', 'C507', 'C508', '0958','C516', '0925', '0922', '0907', 'C524','C593', 'C601', 'M542', 'C609', 'C525', 'C603', 'C530', '0901','C532', 'C533', 'C606', '0965','C540', 'C571', 'C543', 'C596', 'C546', 'C547', '0909','C522', '0962','C554', 'C553', 'C523', 'C608', 'C567', 'C568', '0943','C558', 'C599', 'C565', 'C564', 'C569', '1002', 'C521', '0995','C595', 'C579', 'C573', 'C542', '0963', 'C574', 'C577']\n",
        "\n",
        "idDict = {}\n",
        "for i in range(len(toParse)):\n",
        "  idDict[toParse[i]] = nonParsed[i]\n",
        "\n",
        "\n",
        "url_part1 = \"http://publicreporting.ltchomes.net/en-ca/homeprofile.aspx?Home=\"\n",
        "url_part3 = \"&tab=0\"\n",
        "rows = []\n",
        "notParsed = []\n",
        "for id in toParse:\n",
        "  print(\"Working on\", id)\n",
        "  URL = url_part1 + id + url_part3\n",
        "  page = requests.get(URL)\n",
        "  #Not successful query\n",
        "  if page.status_code != 200:\n",
        "    print(\"Error fetching page\", id)\n",
        "    notParsed.append(id)\n",
        "  #Successful query\n",
        "  else:\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "    all = soup.find_all(class_=\"Profilerow_col2\")\n",
        "    if (all == []):\n",
        "      print(\"Error wt idn page\", id)\n",
        "      notParsed.append(id)\n",
        "    else:\n",
        "      homeType = all[5].getText()\n",
        "      ResCouncil = all[8].getText()\n",
        "      FamCouncil = all[9].getText()\n",
        "      Accreditation = all[10].getText()\n",
        "      id = idDict[id]\n",
        "      row = [id, homeType, ResCouncil, FamCouncil, Accreditation]\n",
        "      print(row)\n",
        "      rows.append(row)\n",
        "\n",
        "#LTC_accreditation\n",
        "with open('part2.csv', 'a') as f:\n",
        "    writer = csv.writer(f)\n",
        "    #writer.writerow([\"LTCH_Num\", \"homeType\", \"ResCouncil\", \"FamCouncil\", \"Accreditation\"])\n",
        "    for row in rows:\n",
        "      writer.writerow(row)\n",
        "\n",
        "files.download('part2.csv') \n",
        "print(notParsed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97U41wZGL0b9"
      },
      "source": [
        "### Part 2: Scraping Non-compliance Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odDLvpIsG_ms"
      },
      "source": [
        "# NON COMPLIANCE \n",
        "\n",
        "nonParsed = ['2983', '3052', '956', '2985', '2986', '2987', '958', '2988', '925',  '922',  '907', '2992', '3023', '3029', '3050', '3034', '2993', '3031', '2995', '901', '2996', '2997', '3035', '965', '3000', '3016', '3002', '3026', '3003', '3004', '909', '2990', '962', '3006', '3005', '2991', '3033', '3013', '3014', '943', '3009', '3028', '3012', '3011', '3015', '3051', 'M633', '995', '3025', '3021', '3017', '3001', '963', '3018', '3020']\n",
        "toParse =   ['C501', '2647', '0956','C506', 'C507', 'C508', '0958','C516', '0925', '0922', '0907', 'C524','C593', 'C601', 'M542', 'C609', 'C525', 'C603', 'C530', '0901','C532', 'C533', 'C606', '0965','C540', 'C571', 'C543', 'C596', 'C546', 'C547', '0909','C522', '0962','C554', 'C553', 'C523', 'C608', 'C567', 'C568', '0943','C558', 'C599', 'C565', 'C564', 'C569', '1002', 'C521', '0995','C595', 'C579', 'C573', 'C542', '0963', 'C574', 'C577']\n",
        "\n",
        "\n",
        "url_part1 = \"https://apps.mohltc.ca/ltchomes/detail.php?id=\"\n",
        "url_part3 = \"&lang=en\"\n",
        "rows = []\n",
        "notParsed = []\n",
        "for id in homeIDs:\n",
        "  print(\"Working on\", id)\n",
        "  URL = url_part1 + id + url_part3\n",
        "  page = requests.get(URL)\n",
        "  #Not successful query\n",
        "  if page.status_code != 200:\n",
        "    print(\"Error fetching page\", id)\n",
        "    notParsed.append(id)\n",
        "  #Successful query\n",
        "  soup = BeautifulSoup(page.content, 'html.parser')\n",
        "  all = soup.find_all('td')\n",
        "  if (all == []):\n",
        "    print(\"Error wt idn page\", id)\n",
        "    notParsed.append(id)\n",
        "  else:\n",
        "    an_order = all[0].getText().split(\" \")[0]\n",
        "    an_nc = all[2].getText().split(\" \")[0]\n",
        "    tar_order = all[4].getText().split(\" \")[0]\n",
        "    tar_nc = all[6].getText().split(\" \")[0]\n",
        "    tar_num = all[8].getText()\n",
        "    row = [id, an_order, an_nc, tar_order, tar_nc, tar_num]\n",
        "    print(row)\n",
        "    rows.append(row)\n",
        "  \n",
        "\n",
        "print(len(notParsed), notParsed)\n",
        "print(len(rows))\n",
        "\n",
        "#LTC_accreditation\n",
        "# with open('compliance1.csv', 'a') as f:\n",
        "#     writer = csv.writer(f)\n",
        "#     #writer.writerow([\"LTCH_Num\", \"homeType\", \"ResCouncil\", \"FamCouncil\", \"Accreditation\"])\n",
        "#     for row in rows:\n",
        "#       writer.writerow(row)\n",
        "\n",
        "# files.download('compliance1.csv') \n",
        "# print(notParsed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEpys5ONODag"
      },
      "source": [
        "# Scratch \n",
        "\n",
        "#LTC_accreditation\n",
        "columns = [\"LTCH_Num\", \"Annual_Orders\", \"Annual_Non-compliance\", \"Targeted_Inspect_Num\", \"Targeted_Orders\", \"Targeted_Non-compliance\"]\n",
        "with open('nonCompliance.csv', 'a') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(columns)\n",
        "    for row in rows:\n",
        "      writer.writerow(row)\n",
        "\n",
        "#files.download('nonCompliance.csv') \n",
        "print(notParsed)\n",
        "\n",
        "notParsed = ['3052', '956', '2930', '2620', '958', '925', '922', '907', '1041', '2513', '2706', '2995', '901', '2974', '965', '2211', '3026', '909', '2693', '962', '2805', '2629', '943', '1135', '2762', '3051', '995', '963', '2803']\n",
        "\n",
        "#Data is incomplete, manual review was conducted for all "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sayAoS81jeEL"
      },
      "source": [
        "### References: \n",
        "1. https://realpython.com/beautiful-soup-web-scraper-python/ \n",
        "2. https://www.scrapingbee.com/blog/python-web-scraping-beautiful-soup/"
      ]
    }
  ]
}